{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHARK QUEST\n",
    "## New Kids On The Block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import depedencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cleaning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "pd.set_option('display.max_rows', 80)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "shark_attacks_df = pd.read_excel('GSAF5.xls')\n",
    "shark_attacks = shark_attacks_df.copy()\n",
    "\n",
    "countries_df = pd.read_csv('country_coord.csv')\n",
    "countries = countries_df.copy()\n",
    "\n",
    "shark_attacks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format all columns names\n",
    "shark_attacks.columns = [col.strip().replace(\" \", \"_\").replace(\".\", \"\").lower() for col in shark_attacks.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contract\n",
    "data_schema = dictionnary_from_json('schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns\n",
    "shark_attacks['severity'] = shark_attacks_df['Injury']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns\n",
    "shark_attacks = shark_attacks[[col for col in data_schema]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip strings\n",
    "shark_attacks = shark_attacks.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "shark_attacks.replace(['N/A', 'null', '--'], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "shark_attacks.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index\n",
    "shark_attacks.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat values\n",
    "replacements = dictionnary_from_json('replacements')\n",
    "\n",
    "for col, values in replacements.items():\n",
    "    shark_attacks[col] = shark_attacks[col].apply(\n",
    "        replace_string_patterns, replacements=values)\n",
    "    \n",
    "uncaught = shark_attacks['severity'][~shark_attacks['severity'].isin(['INJURY','FATALITY'])] \n",
    "uncaught"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat country\n",
    "shark_attacks['country'] = shark_attacks['country'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "replace_to_nan(shark_attacks[\"country\"], [\"england\",\"scotland\"], \"united kingdom\")\n",
    "replace_to_nan(shark_attacks[\"country\"], [\"usa\", \"hawaii\"], \"united states\")\n",
    "replace_to_nan(shark_attacks[\"country\"], [\"reunion\"], \"france\")\n",
    "replace_to_nan(shark_attacks[\"country\"], [\"columbia\"], \"colombia\")\n",
    "replace_to_nan(shark_attacks[\"country\"], [\"new guinea\"], \"papua new guinea\")\n",
    "\n",
    "\n",
    "# unify country, converts all low value \"country\" ocorrences into <NA>. Also sets Country as string-type\n",
    "country_list = [x.lower() for x in countries['Country']]\n",
    "shark_attacks[\"country\"] = shark_attacks[\"country\"].where(shark_attacks[\"country\"].isin(country_list), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dates\n",
    "shark_attacks['date'] = pd.to_datetime(shark_attacks['date'], errors='coerce') #Drops unsavable mess\n",
    "shark_attacks['date'] = shark_attacks['date'].dt.strftime('%d-%m-%Y') \n",
    "shark_attacks['date'] = shark_attacks['date'].ffill() #Fills forward to avoid time gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast correct types\n",
    "columns_types = {key: value['dtype'] for key, value in data_schema.items()}\n",
    "shark_attacks = shark_attacks.astype(columns_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean categories\n",
    "for column in shark_attacks.select_dtypes(include=['category']).columns:\n",
    "    shark_attacks[column] = shark_attacks[column].astype('string')\n",
    "    shark_attacks[column] = pd.Categorical(shark_attacks[column], categories=set(data_schema[column]['categories']), ordered=True)\n",
    "    shark_attacks[column] = shark_attacks[column].where(shark_attacks[column].isin(data_schema[column]['categories']), other=data_schema[column]['categories'][-1])\n",
    "    shark_attacks[column] = shark_attacks[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add helper columns\n",
    "shark_attacks['severity_score'] = shark_attacks['severity'].apply(lambda x: 3 if x == 'FATALITY' else 2 if x == 'INJURY' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean copy for analysis\n",
    "shark_attacks_clean = shark_attacks.copy()\n",
    "shark_attacks_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis: Shark attacks are more concentrated in the PM \n",
    "time_stats = shark_attacks['time'].cat.remove_categories('UNKNOWN')\n",
    "\n",
    "time_counts = time_stats.value_counts()\n",
    "time_counts\n",
    "\n",
    "\n",
    "df_time = pd.DataFrame(\n",
    "    {'Time Category': time_counts.index,\n",
    "     'Number of Attacks': time_counts.values  \n",
    "    }\n",
    ")\n",
    "df_time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.barplot(x='Time Category', y='Number of Attacks', data=df_time, palette='Blues')\n",
    "plt.title('Shark Attacks Concentration by Time of Day')\n",
    "plt.xlabel('Time Category')\n",
    "plt.ylabel('Number of Attacks')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#Conclusion: we could easily determine that shark attacks are most concentrated during NOON based on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPOTHESIS: SOME SHARK SPECIES ARE MORE DANGEROUS THAN OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by species and severity score, count occurences for each species\n",
    "species_severity_count = shark_attacks_clean.groupby(['species', 'severity_score']).size().unstack(fill_value=0)\n",
    "\n",
    "#drop MYSTERY SHARK row\n",
    "species_severity_count = species_severity_count.drop(\"MYSTERY SHARK\")\n",
    "\n",
    "#reorder the columns\n",
    "severity_order = [1, 2, 3]\n",
    "species_severity_count = species_severity_count[severity_order]\n",
    "\n",
    "\n",
    "#calculate the percentage\n",
    "total_counts = species_severity_count.sum(axis=1)\n",
    "\n",
    "percentage_severity = species_severity_count.div(total_counts, axis=0) * 100\n",
    "\n",
    "\n",
    "#Filter out low incidence species\n",
    "species_with_high_counts = total_counts[total_counts > 15].index\n",
    "\n",
    "filtered_species = percentage_severity.loc[species_with_high_counts]\n",
    "\n",
    "#convert to string so seaborn correctly filters out dropped species\n",
    "filtered_species.index = filtered_species.index.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by occurence of FATALITY\n",
    "sorted_by_category_3 = filtered_species.sort_values(by=3, ascending=False)\n",
    "\n",
    "top_severity_3 = sorted_by_category_3.head(6)\n",
    "top_severity_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOST DANGEROUS SHARK SPECIES VISUALISATION\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "top_severity_3 = top_severity_3.reset_index()\n",
    "\n",
    "# Create Bar Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_severity_3[3], y=top_severity_3['species'], palette='viridis')\n",
    "\n",
    "plt.title('Most Dangerous Shark Species!')\n",
    "plt.xlabel('Severity Score 3 (%)')\n",
    "plt.ylabel('Species')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by occurence of NO-INJURY\n",
    "top_severity_1 = filtered_species.sort_values(by=1, ascending=False)\n",
    "\n",
    "#filter for lower than 5% fatality rate:\n",
    "top_severity_1_notdeadly = top_severity_1[top_severity_1[3]<5].head(6)\n",
    "display(top_severity_1_notdeadly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRIENDLIEST SHARK SPECIES VISUALISATION\n",
    "\n",
    "top_severity_1_notdeadly = top_severity_1_notdeadly.reset_index()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_severity_1_notdeadly[1], y=top_severity_1_notdeadly['species'], palette='viridis')\n",
    "\n",
    "plt.title('Friendliest Shark Species :)')\n",
    "plt.xlabel('Severity Score 1 (%)')\n",
    "plt.ylabel('Species')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPOTHESIS: SOME COUNTRIES HAVE HIGHER PROVOKED / TOTAL ATTACK RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## working with the clean dataframe instead of main, clearing unused information\n",
    "sa_clean = shark_attacks_clean[[\"date\", \"time\", \"country\", \"state\", \"location\", \"type\", \"severity\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS REQUIRED, to be sent to a different file\n",
    "\n",
    "\n",
    "def group_and_filter_by_type(df, groupby_cols, attack_type=None, count_threshold=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Groups the DataFrame by specified columns and applies filter conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to group and filter\n",
    "    - groupby_cols: List of columns to group by\n",
    "    - filter_conditions: A dictionary containing column names and their corresponding filter conditions\n",
    "    \n",
    "    Returns:\n",
    "    - A filtered DataFrame grouped by specified columns with a 'count' column\n",
    "    \"\"\"\n",
    "     # Group by the specified columns and calculate the count of each group\n",
    "    grouped_df = df.groupby(groupby_cols).size().reset_index(name='count')\n",
    "    \n",
    "    # Apply the count filter\n",
    "    grouped_df = grouped_df[grouped_df['count'] > count_threshold]\n",
    "    \n",
    "    # Apply the type filter if attack_type is provided\n",
    "    if attack_type:\n",
    "        grouped_df = grouped_df[grouped_df['type'] == attack_type]\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "def clean_merge_and_plot(p_sa, up_sa, merge_on=\"country\", provoked_col=\"count_x\", unprovoked_col=\"count_y\", drop_columns=None):\n",
    "    \"\"\"\n",
    "    Merges two dataframes, calculates totals and provoked/unprovoked ratios, and visualizes results.\n",
    "\n",
    "    Parameters:\n",
    "    - p_sa: DataFrame for provoked attacks\n",
    "    - up_sa: DataFrame for unprovoked attacks\n",
    "    - merge_on: Column name to merge on (default: 'country')\n",
    "    - provoked_col: Column name for provoked counts (default: 'count_x')\n",
    "    - unprovoked_col: Column name for unprovoked counts (default: 'count_y')\n",
    "    - drop_columns: list of column names to drop, default None\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame sorted by the provoked/unprovoked ratio\n",
    "    - A seaborn plot showing the ratio of provoked attacks by country\n",
    "    \"\"\"\n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.merge(\n",
    "        left=p_sa,\n",
    "        right=up_sa,\n",
    "        on=merge_on,\n",
    "        how='inner'\n",
    "        ).reset_index(drop=True).rename(\n",
    "                                        columns={\n",
    "                                            provoked_col: \"provoked\",\n",
    "                                            unprovoked_col: \"unprovoked\"\n",
    "                                            }\n",
    "                                        )\n",
    "    \n",
    "    # Calculate total and ratio directly on columns (vectorized)\n",
    "    merged_df[\"total\"] = merged_df[\"provoked\"] + merged_df[\"unprovoked\"]\n",
    "    merged_df[\"ratio\"] = (merged_df[\"provoked\"] / merged_df[\"total\"] * 100).round(2)\n",
    "\n",
    "    # Sort by the 'ratio' column in descending order\n",
    "    merged_df = merged_df.sort_values(by=\"ratio\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Drop columns if provided\n",
    "    if drop_columns:\n",
    "        columns_to_drop = [col for col in drop_columns if col in merged_df.columns]\n",
    "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Print the cleaned DataFrame\n",
    "    #display(merged_df)\n",
    "\n",
    "    # Plotting the ratio of provoked attacks by country\n",
    "    sns.catplot(data=merged_df, kind=\"bar\", x=merge_on, y=\"ratio\", hue=\"total\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Provoked / Total attacks\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### whole-time overview\n",
    "\n",
    "# Converts country to lowercase    \n",
    "sa_clean[\"country\"] = sa_clean[\"country\"].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "\n",
    "# introduces country sheet. Converts into a list  ### Convert into dictionary, pass it through repnan or JP's\n",
    "replace_to_nan(sa_clean[\"country\"], [\"england\",\"scotland\", \"cayman islands\", \"bermuda\", \"british virgin islands\", \"british overseas territory\"], \"united kingdom\")\n",
    "replace_to_nan(sa_clean[\"country\"], [\"usa\", \"hawaii\"], \"united states of america\")\n",
    "replace_to_nan(sa_clean[\"country\"], [\"reunion\"], \"france\")\n",
    "replace_to_nan(sa_clean[\"country\"], [\"columbia\"], \"colombia\")\n",
    "replace_to_nan(sa_clean[\"country\"], [\"new guinea\"], \"papua new guinea\")\n",
    "\n",
    "# unify country, converts all low value \"country\" ocorrences into <NA>. Also sets Country as string-type\n",
    "sa_clean[\"country\"] = sa_clean[\"country\"].where(sa_clean[\"country\"].isin(country_list)).astype(\"string\")\n",
    "\n",
    "# this should replace all the bottom part\n",
    "provoked_sa_country_type = group_and_filter_by_type(sa_clean, [\"country\", \"type\"], attack_type=\"PROVOKED\", count_threshold=3)\n",
    "unprovoked_sa_country_type = group_and_filter_by_type(sa_clean, [\"country\", \"type\"], attack_type=\"UNPROVOKED\", count_threshold=4)\n",
    "watercraft_sa_country_type = group_and_filter_by_type(sa_clean, [\"country\", \"type\"], attack_type=\"WATERCRAFT\", count_threshold=0)\n",
    "\n",
    "clean_merge_and_plot(provoked_sa_country_type, unprovoked_sa_country_type, merge_on=\"country\", provoked_col=\"count_x\", unprovoked_col=\"count_y\", drop_columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Lets try to get last 50yrs\n",
    "date_start = \"01-01-1975\"\n",
    "date_start= pd.to_datetime(date_start, format='%d-%m-%Y')\n",
    "sa_clean[\"date\"] = sa_clean[\"date\"].astype('datetime64[ns]')\n",
    "\n",
    "# Makes a new database with only last 50yrs score (start jan-1)    \n",
    "sa_clean_new = sa_clean[sa_clean['date'] > (date_start)].copy()\n",
    "\n",
    "# Apply the group_and_filter function to get the respective DataFrames\n",
    "p_sa_country_type_new = group_and_filter_by_type(sa_clean_new, [\"country\", \"type\"], attack_type=\"PROVOKED\", count_threshold=2)\n",
    "up_sa_country_type_new = group_and_filter_by_type(sa_clean_new, [\"country\", \"type\"], attack_type=\"UNPROVOKED\", count_threshold=4)\n",
    "wc_sa_country_type_new = group_and_filter_by_type(sa_clean_new, [\"country\", \"type\"], attack_type=\"WATERCRAFT\")\n",
    "\n",
    "# Plots the info based on clean and merge function\n",
    "clean_merge_and_plot(p_sa_country_type_new, up_sa_country_type_new, drop_columns=[\"type_x\", \"type_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hipotesis: provoked attack have higher severity than unprovoked attacks // REBUKED\n",
    "\n",
    "sa_sev = group_and_filter_by_type(sa_clean, [\"type\",\"severity\"], attack_type=None, count_threshold=0)\n",
    "\n",
    "#Calculates total counts for each type\n",
    "total_counts = sa_sev.groupby('type')['count'].sum().reset_index(name='total_count')\n",
    "\n",
    "#Merge total counts back \n",
    "sa_sev = sa_sev.merge(total_counts, on='type')\n",
    "\n",
    "#Calculate percentage\n",
    "sa_sev['percentage'] = (sa_sev['count'] / sa_sev['total_count']) * 100\n",
    "\n",
    "# Display the result\n",
    "display(sa_sev[['type', 'severity', 'count', 'total_count', 'percentage']])\n",
    "\n",
    "sns.catplot(data=sa_sev, kind=\"bar\", x=\"severity\", y=\"percentage\",hue=\"type\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Provoked / Total attacks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source, PDF, Case Number as possible duplicate finder or year / date fill\n",
    "\n",
    "# Henning : Date, Type\n",
    "# Ricardo : Country, State\n",
    "# Linh : Location, Activity\n",
    "# Jp : Injury, Time\n",
    "\n",
    "# Type : category : Merge some columns based on categories. Trim labels. Nan into invalid. Final => Provoked, Unprovoked, Invalid\n",
    "# Date : datetime : Clean \"Reported\" - Harmonize Format - Cast weird into NaT. Final => Dates (as datetime), NaT\n",
    "# Country : string : Strip spaces - Formatting - Replace weird characters - cast weird values as NaN => Strings, Nan\n",
    "# State : string : Strip spaces - Formatting - Replace weird characters - cast weird values as NaN => Strings, Nan\n",
    "# Location : string : Strip spaces - Formatting - Replace weird characters - cast weird values as NaN => Strings, Nan\n",
    "# Activity : category : Merge some columns based on categories. Trim labels. Nan into invalid. Final => Few categories to be determined\n",
    "# Injury : category : Merge columns based on keywords. Nan into other. Final => Fatality, Injury, Other\n",
    "# Time : category : Cast into categories Final => morning (6-10) noon (10-14) afternoon( 14-18) dusk (18-22) night (22 - 2) dawn (2-6) maybe as integers (0-5)\n",
    "\n",
    "# Todo\n",
    "# Remove obvious duplicate (entire line) - Ricardo\n",
    "# Remove fuzzy duplicates (case number? dates?) - Ricardo\n",
    "\n",
    "# Functions :\n",
    "# Merge categories : (*categories to be merged, target) - Henning\n",
    "# Strip function : strips spaces - Linh\n",
    "# Replace as Nan, Nat, ... function - Ricardo\n",
    "# DONE - Replace by keyword function - Jp\n",
    "# DONE - Cast to dateTime function\n",
    "# Matching function (find similarities, keyword based?)\n",
    "# DONE - Reformat dates, strings\n",
    "# Filter function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting\n",
    "- Select relevant columns\n",
    "- Analyse relevant columns\n",
    "\n",
    "## Cleaning :\n",
    "- Cast to appropriate data types\n",
    "    - General cleaning\n",
    "        - identify duplicates\n",
    "            - fuzzy\n",
    "        - removing duplicates\n",
    "            - remove\n",
    "            - merge\n",
    "        - handling null values\n",
    "            - remove\n",
    "            - replace\n",
    "        - manipulating strings\n",
    "        - formatting the data.\n",
    "\n",
    "- Wrong inputs\n",
    "- Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERAL CLEANING\n",
    "1. Rename columns\n",
    "2. Drop columns\n",
    "3. Remove duplicates\n",
    "    - Remove full dupes\n",
    "    - Remove fuzzy search\n",
    "4. Strip values\n",
    "5. Reset Index\n",
    "\n",
    "### SPECIFIC CLEANING\n",
    "1. Search / Replace / Reformat strings\n",
    "2. Merge categories\n",
    "3. Cast to Null\n",
    "4. Cast correct type\n",
    "5. Create new columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "### TIME\n",
    "- Shark attacks are seasonal (Summer)\n",
    "- Shark attacks are increasing \n",
    "- Shark attacks are more concentrated in the PM \n",
    "\n",
    "### LOCATION\n",
    "- Some countries are more attack prone (Australia)\n",
    "- Some countries are more likely to be fatal (Australia)\n",
    "\n",
    "\n",
    "### DEMOGRAPHICS\n",
    "- Males are more likely to get attacked\n",
    "- Males are more likely to get provoke a shark\n",
    "- Provoked attacked are more fatal\n",
    "- Young persons are more likely to get attacked\n",
    "- Old persons are more likely to get killed\n",
    "\n",
    "- Names more likely to get attacked (John)\n",
    "\n",
    "### OTHER\n",
    "- Some species are more aggressive (Tiger Shark)\n",
    "- Some activities are more likely (Surfing)\n",
    "- Some activities are more fatal\n",
    "\n",
    "- Full moon? 😂\n",
    "\n",
    "\n",
    "Retained :\n",
    "- Shark Species have a gender preference (score) => JP => Heinning\n",
    "- Some countries are more provocative against sharks* => Ricardo\n",
    "- Shark attacks are more concentrated in the PM => Linh\n",
    "\n",
    "- Names more likely to get attacked (John) => JP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Ideas\n",
    "\n",
    "- App that gives a likelyhood of attack based on location and time\n",
    "- Vacations far away from sharks for phobics\n",
    "- Witness attacks for masochists\n",
    "\n",
    "- Surf school at the safest places / seasons\n",
    "- Fishing supplies => shark repellant by activities\n",
    "- Safety training to avoid provocations / live in harmony\n",
    "\n",
    "- Shark repellant => best spots\n",
    "- Insurance for surfers, premiums for high risk areas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
